{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node class decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the structure of a decision tree, it is composed of numerous nodes which is either the parent node or the leaf node. Then it is convenient to construct a object denoting the node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Attributes: \n",
    "    * in the case of binary split, the node class at least should has two attributes (denoted as self.true and self.false here) pointing to its two child nodes.\n",
    "    * furthermore, it should be able to tell which child node a given data should go to. Thus information regarding the binary split should also be stored i.e. self.feature, self.dtype, self.threshold. dtype tells whether the feature involved in the split is discrete, categorical (dtype=='d') or continuous,ordinal (dtype='c').\n",
    "    * additionally self.probability is the ratio of subsample size in current node to the total sample size. self.sample_index stores the index of the subsample in the total sample.\n",
    "2. methods:\n",
    "    * find child method returns either child node given input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, if the node is leaf, it should be able to give a prediction according to the model on that leaf node. In the case of regression, this model can be e.g. the average of outcome, a linear regression, or more complex models. Thus without losing generality, a tree node decorator is defined, it takes a model class as argument and use it as a parent class to build a node class on top of it. Following is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a tree node decorator: takes in a model class and create a child class out of it, then return the child class\n",
    "#  to create a tree_node class e.g. node=tree_node_decorator(linear_regression)()\n",
    "def tree_node_decorator(model,hparameter=None,stats=None):\n",
    "    class node_model(model):\n",
    "        def __init__(self,hparameter=hparameter,stats=stats):\n",
    "\n",
    "            if not hparameter:\n",
    "                super().__init__(stats=stats)              \n",
    "            else:\n",
    "                super().__init__(hparameter=hparameter,stats=stats)\n",
    "\n",
    "\n",
    "            # tree_node: the left or true branch \n",
    "            self.true=None \n",
    "\n",
    "            # tree_node: the right or false branch\n",
    "            self.false=None\n",
    "\n",
    "\n",
    "\n",
    "            # parameters used in the split method: return true if the split criterion suggest going to the true child node otherwise false\n",
    "            self.feature=None       \n",
    "            self.dtype=None\n",
    "            self.threshold=None\n",
    "            \n",
    "            # probability that a instance falls into current node\n",
    "            self.probability=1\n",
    "            \n",
    "            # store the index of the subsamples belong to the current node in the total training sample\n",
    "            self.sample_index=None\n",
    "\n",
    "\n",
    "        def find_child(self,unit):\n",
    "            '''for each unit return true or false according to the criteria'''\n",
    "            if self.dtype=='c':\n",
    "                return self.true if unit[self.feature] <= self.threshold else self.false\n",
    "\n",
    "            elif self.dtype=='d':\n",
    "                 return self.true if unit[self.feature] == self.threshold else self.false\n",
    "\n",
    "            else:\n",
    "                raise ValueError('The datatype of feature is incorrect. \\'c\\' for continuous feature \\\n",
    "                        and \\'d\\' for discrete feature')\n",
    "    return node_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give a typical structure of a model that can be incorporated on the tree node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plain_regression():\n",
    "    def __init__(self,hyperparameter=0,stats=[None,None]):\n",
    "    \n",
    "    def online_fit(self,X,Y,feature_i):\n",
    "           \n",
    "    def batch_fit(self,X,Y,feature_i):            \n",
    "                  \n",
    "    def fit(self,X,Y):\n",
    "   \n",
    "    def predict(self,X):   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is the online_fit method. It takes input the sample data and output two list: stats_up, stats_down. The $i$th element in stats_up is a list: $[parameter,loss,status variable]$ which describesthat if the model is trained on the subsample $X[:i+1]$ what is the parameter of the trained model, the loss function after traning, and the state variables which assistants the online training or updating the other two quantities. The $i$th element in stats_down describes same quantities if trained on the subsample $X[i:]$ instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tree class decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree class decorator takes in the tree_node class built from previous discussion and return a decision tree class.\n",
    "\n",
    "**Methods** (briefly): \n",
    "\n",
    "* main methods: \n",
    "    The **fit** method of the decision_tree class build a tree given input data recursively by using the **build_tree** method, then the **pruning** method is applied automatically to get rid of the leaves that do not contribute to a significant amount of loss reduction. **predict** method return the corresponding prediction given data.\n",
    "* other methods: \n",
    "    export_graphviz and plot_tree methods are to visualize the tree structure using the package graphviz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopping criteria**\n",
    "\n",
    "It is important for the recursive building tree method to know when to stop. It is controlled by the two attributes: self.max_depth and self.min_sample_size. Here when the sample size is smaller than twice of the min_sample_size or the splitting results in a subsample smaller than the min_sample_size the splitting is stopped and current node is a leaf node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruning criteria**\n",
    "\n",
    "in the tree pruning process, whether the splitting on the parent node of two leaf nodes is kept or not depending the following formula\n",
    "$$\n",
    "\\mathcal{L}_1 \\frac{N_1}{N}+\\gamma + \\mathcal{L}_2 \\frac{N_2}{N}+\\gamma\\leq (\\mathcal{L}+\\gamma)\\lambda\n",
    "$$\n",
    "where again $\\mathcal{L}_1$ and $\\mathcal{L}_2$ are the two leaf node loss functions and $N_1$, $N_2$ their sample sizes. $\\mathcal{L}$ is the parent node loss function and $N$ is the parent node sample size. $\\gamma$ is a regularization parameter assign additional punishment to each split. $\\lambda$ is a ratio controlling how much loss reduction is needed for the split to be significant enough. They correspond to the self.gamma and self.loss_red_thres attributes respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimization**\n",
    "\n",
    "The most time consuming part of the tree algorithms is to find the threshold of splitting for ordinal feature. Since for ordinal feature, the splitting criteria is whether $X\\leq thershold$, the brutal force algorithm takes $O(N^2)$ to both find the optimal threshold where $N$ is the number of values of that feature without considering the fitting time of the node model. However, if the data is sorted first, it only takes $O(N\\log N)$. Thus it is optimized in time if the data is first sorted by feature. However, the data size is enlarged since we now have $d$ times the original data set with $d$ the number of features. \n",
    "\n",
    "This may cause a problem if the data set is too large to fit in the memory at once. However, there is a work around if the model used on the node support online training or batch training. Because now the data can be stored on hard disk and read into memory in the form of batches or sequence. At least in the case of linear regression and a model by just averaging outcome such online training or batch training method exists. The discussion can be find [here](../../Online_ML/OML_theory.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a decision tree decorator: takes in a node class and create a child class out of it, then return the child class\n",
    "#  to create a tree_node class e.g. node=tree_node_decorator(linear_regression)()\n",
    "\n",
    "def decision_tree_decorator(tree_node,max_depth=np.Infinity,min_sample_size=2,gamma=0,loss_red_thres=0.9):\n",
    "    class decision_tree():\n",
    "        \"\"\"\n",
    "        X=np.array(shape=[samplesize,feature])\n",
    "        Y=np.array(shape=[samplesize])\n",
    "        feature_name_dtype=np.array([(name,'c'/'d')...])\n",
    "        where 'c' respresents continuous variabl and 'd' represents discrete variable\n",
    "        \"\"\"\n",
    "        def __init__(self,max_depth=max_depth,min_sample_size=min_sample_size,gamma=gamma):\n",
    "            self.max_depth=max_depth\n",
    "            self.min_sample_size=min_sample_size\n",
    "            \n",
    "            self.gamma=gamma\n",
    "            self.loss_red_thres=loss_red_thres\n",
    "                                 \n",
    "\n",
    "\n",
    "            # quantities used in building tree\n",
    "            self.root=None\n",
    "            self.feature_name_dtype=None\n",
    "\n",
    "        def fit(self,X=[],Y=[],XY_mul=[],feature_name_dtype=None):\n",
    "            '''\n",
    "            feature_name_dtype=[feature_name,'c' for ordinal feature and 'd' for categorical feature]\n",
    "            hparameter: hyperparameters for the model of on the tree node \n",
    "            '''\n",
    "            self.feature_name_dtype=np.array(feature_name_dtype)\n",
    "            \n",
    "            if len(X)!=0 and len(Y)!=0:\n",
    "        \n",
    "                # just to store the root of the tree \n",
    "                # consistent check whether the shape of inputs are expected:\n",
    "                condition1= ((len(np.shape(X))==2)\n",
    "                            and (len(np.shape(self.feature_name_dtype))==2))         \n",
    "                condition2= ((np.shape(X)[0]==np.shape(Y)[0]) \n",
    "                             and (np.shape(X)[1]==np.shape(self.feature_name_dtype)[0]))\n",
    "\n",
    "                if condition1 and condition2:\n",
    "                    if len(np.shape(Y))==1: \n",
    "                        Y = np.expand_dims(Y, axis=1)\n",
    "\n",
    "                    # Add Y,g,h as last columns to X so easier to split them together\n",
    "                    XY = np.concatenate((X, Y), axis=1)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError('The shapes of inputs are not compatible')\n",
    "\n",
    "                # store data \n",
    "                self.XY_mul=np.array([sorted(XY,key=lambda unit: unit[i]) for i in range(X.shape[1])])                               \n",
    "                \n",
    "            elif len(XY_mul)!=0:\n",
    "                self.XY_mul=XY_mul\n",
    "                \n",
    "            else:\n",
    "                raise ValueError('No input data is given')\n",
    "                \n",
    "            currnode_index=np.ones(self.XY_mul.shape[:-1],dtype=bool)\n",
    "                                       \n",
    "            self.root=self.build_tree(currnode_index=currnode_index)\n",
    "\n",
    "            # pruning trees\n",
    "            self.pruning(self.root)\n",
    "\n",
    "            # release memory\n",
    "            self.XY_mul=0\n",
    "\n",
    "\n",
    "        def build_tree(self,currnode_index=True,depth=1,stats=None,probability=1):   \n",
    "            \n",
    "            \n",
    "            # pick the units out of the total samples that correspond to current node determined by the currnode_index\n",
    "            XY_mul=self.XY_mul[currnode_index].reshape(len(self.feature_name_dtype),-1,len(self.feature_name_dtype)+1)\n",
    "            \n",
    "            parent_condition=((depth<self.max_depth) and (XY_mul.shape[1]>=2*self.min_sample_size))\n",
    "            \n",
    "            leaf_condition=((depth==self.max_depth) or (XY_mul.shape[1]<2*self.min_sample_size))\n",
    "            \n",
    "            null_condtion=((depth>self.max_depth) or (XY_mul.shape[1]<self.min_sample_size))\n",
    "            \n",
    "            if null_condtion:\n",
    "                return None\n",
    "            \n",
    "            elif leaf_condition:\n",
    "                node=tree_node(stats=stats)\n",
    "                node.fit(XY_mul[0,:,:-1],XY_mul[0,:,-1:])             \n",
    "                node.probability=probability\n",
    "                node.sample_index=currnode_index\n",
    "                \n",
    "                return node\n",
    "                    \n",
    "            elif parent_condition:    \n",
    "                # this node has the potential to be parent\n",
    "                node=tree_node(stats=stats)                    \n",
    "                node.probability=probability\n",
    "                node.sample_index=currnode_index\n",
    "\n",
    "                best=[np.Infinity,0,0]  ##[best split loss, feature_i whic gives best split, value of feature_i gives best split]\n",
    "                best_split_stats=[[],[]]     ##[left split stats,right split stats]\n",
    "                best_split_left_size=0\n",
    "                best_split_right_size=0\n",
    "\n",
    "                for feature_i in range(len(XY_mul)):\n",
    "                    if self.feature_name_dtype[feature_i][1]=='c':\n",
    "                        b_split_left_stats,b_split_right_stats,b_split_left_size,b_split_right_size,b_split_value,b_loss\\\n",
    "                        =node.online_fit(XY_mul[feature_i,:,:-1],XY_mul[feature_i,:,-1:],feature_i,feature_name_dtype=self.feature_name_dtype)\n",
    "                    elif self.feature_name_dtype[feature_i][1]=='d':\n",
    "                        b_split_left_stats,b_split_right_stats,b_split_left_size,b_split_right_size,b_split_value,b_loss\\\n",
    "                        =node.batch_fit(XY_mul[feature_i,:,:-1],XY_mul[feature_i,:,-1:],feature_i,feature_name_dtype=self.feature_name_dtype)\n",
    "\n",
    "                    if b_loss<best[0]:\n",
    "                        best[0]=b_loss\n",
    "                        best[1]=feature_i\n",
    "                        best[2]=b_split_value\n",
    "\n",
    "                        best_split_stats[0]=b_split_left_stats\n",
    "                        best_split_stats[1]=b_split_right_stats\n",
    "                        \n",
    "                        best_split_left_size=b_split_left_size\n",
    "                        best_split_right_size=b_split_right_size\n",
    "                        \n",
    "                # whether disorder reduction is significant enough to support the division\n",
    "                condition=(best_split_left_size>=self.min_sample_size) and (best_split_right_size>=self.min_sample_size)\n",
    "                \n",
    "                if condition: \n",
    "                # this is truly a parent node\n",
    "                    if self.feature_name_dtype[best[1],1]=='c':\n",
    "                        node.dtype='c'        \n",
    "                        left_index=(self.XY_mul[:,:,best[1]]<=best[2]) & (currnode_index)\n",
    "                        right_index=(self.XY_mul[:,:,best[1]]>best[2]) & (currnode_index)\n",
    "                    else:\n",
    "                        node.dtype='d'\n",
    "                        left_index=(self.XY_mul[:,:,best[1]]==best[2]) & (currnode_index)\n",
    "                        right_index=(self.XY_mul[:,:,best[1]]!=best[2]) & (currnode_index)\n",
    "\n",
    "                    node.feature=best[1]\n",
    "                    node.threshold=best[2]\n",
    "                    \n",
    "                    \n",
    "                    pro_left=best_split_left_size/(best_split_left_size+best_split_right_size)*node.probability\n",
    "                    pro_right=best_split_right_size/(best_split_left_size+best_split_right_size)*node.probability\n",
    "                    \n",
    "                    ## add child branches\n",
    "                    node.true=self.build_tree(currnode_index=left_index,depth=depth+1,stats=best_split_stats[0],probability=pro_left)\n",
    "                    node.false=self.build_tree(currnode_index=right_index,depth=depth+1,stats=best_split_stats[1],probability=pro_right)                       \n",
    "                \n",
    "                return node \n",
    "            \n",
    "        def pruning(self,currnode):\n",
    "            if currnode.true is None:\n",
    "                return currnode\n",
    "            else:\n",
    "                # prune child nodes first\n",
    "                currnode.true=self.pruning(currnode.true)\n",
    "                currnode.false=self.pruning(currnode.false)\n",
    "                \n",
    "                # whether current node is the parent of two leaf nodes\n",
    "                if currnode.true.true is None and currnode.false.false is None:\n",
    "                    loss_leaves=(currnode.true.loss*currnode.true.probability\\\n",
    "                                +currnode.false.loss*currnode.false.probability+2*self.gamma)\n",
    "                    loss_parent=currnode.loss*currnode.probability+self.gamma\n",
    "                    \n",
    "                    # whether loss reduction is too small: need to be removed\n",
    "                    if (loss_parent!=0) and (loss_leaves/loss_parent>=self.loss_red_thres):\n",
    "                        currnode.true=None\n",
    "                        currnode.false=None\n",
    "                return currnode\n",
    "            \n",
    "        def export_graphviz(self):\n",
    "            # BFS trasverse the tree structure         \n",
    "            def get_name(currnode):\n",
    "                if not (currnode.true is None):                \n",
    "                    if currnode.dtype=='c':\n",
    "                        curr_name='{}<={}'.format(self.feature_name_dtype[currnode.feature][0],currnode.threshold)\n",
    "                    else:\n",
    "                        curr_name='{} is {}'.format(self.feature_name_dtype[currnode.feature][0],currnode.threshold)\n",
    "                else:\n",
    "                    curr_name=currnode.model_description()\n",
    "                return curr_name\n",
    "            \n",
    "            dot = Digraph(comment='Decision Tree')\n",
    "            que=[self.root]\n",
    "            currnode=que[0]\n",
    "            parent_name=get_name(currnode)\n",
    "            dot.node(parent_name)\n",
    "            \n",
    "            while que:\n",
    "                currnode=que[0]\n",
    "                parent_name=get_name(currnode)\n",
    "                if not (que[0].true is None):\n",
    "                    currnode=que[0].true\n",
    "                    true_child_name=get_name(currnode)\n",
    "                    dot.node(true_child_name)\n",
    "                    dot.edge(parent_name,true_child_name)\n",
    "                    que.append(currnode)\n",
    "                    \n",
    "                    currnode=que[0].false\n",
    "                    false_child_name=get_name(currnode)                  \n",
    "                    dot.node(false_child_name)\n",
    "                    dot.edge(parent_name,false_child_name)\n",
    "                    que.append(currnode)                            \n",
    "         \n",
    "                que.pop(0)\n",
    "            \n",
    "            return dot\n",
    "                                   \n",
    "        def plot_tree(self):\n",
    "            self.export_graphviz().render(view=True)\n",
    "            \n",
    "\n",
    "\n",
    "        def predict(self,X):\n",
    "            if self.root==None:\n",
    "                raise Exception('model not fitted yet')\n",
    "            else:\n",
    "                predictions=np.array([])\n",
    "                if len(np.shape(X))==2 and np.shape(X)[1]==len(self.feature_name_dtype):\n",
    "                    for instance in X:\n",
    "                        currnode=self.root\n",
    "                        while currnode.true!=None: # the current node has branches\n",
    "                            currnode=currnode.find_child(instance)\n",
    "                        predictions=np.concatenate((predictions,currnode.predict(instance)),axis=0)  \n",
    "                    return predictions\n",
    "                else:\n",
    "                    raise ValueError('the shape of X should be (sample,feature)')\n",
    "         \n",
    "            \n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class average_regression():\n",
    "    def __init__(self,hparameter=0,stats=None):\n",
    "        self.lambd=hparameter\n",
    "        if stats is None:\n",
    "            self.parameter=0     \n",
    "            self.loss=0\n",
    "        else:\n",
    "            self.parameter=stats[0]      \n",
    "            self.loss=stats[1]\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    def online_fit(self,X,Y,feature_i,feature_name_dtype=None):\n",
    "        '''calculate stats for every binary division of the XY into XY[:i] and XY[i:] with i in range(len(XY))\n",
    "           stats_up: store information about subsample XY[:i]\n",
    "           stats_down: store information about subsample XY[i:]\n",
    "        '''\n",
    "        ## quantites record information about the best split\n",
    "        N=len(Y)\n",
    "        \n",
    "        b_split_value=None    \n",
    "        b_loss=np.Infinity  \n",
    "        b_split_left_size=1\n",
    "        b_split_right_size=N-1\n",
    "        \n",
    "        b_split_left_stats=None\n",
    "        b_split_right_stats=None\n",
    "               \n",
    "        \n",
    "        \n",
    "        # initialize stats_up\n",
    "        stats_up=[[] for i in range(len(Y))] \n",
    "        stats_up[0].append(self.partial_fit(X[0:1],Y[0:1]))                       # value of omega\n",
    "        stats_up[0].append(self.loss_cal(X[0:1],Y[0:1]))                          # value of loss\n",
    "\n",
    "       \n",
    "        # initialize stats_down\n",
    "        if not self.parameter:\n",
    "            self.fit(X,Y)\n",
    "        if not self.loss:\n",
    "            self.loss=self.loss_cal(X,Y)\n",
    "\n",
    "        stats_down=[[] for i in range(len(Y))]\n",
    "        stats_down[0].append(self.parameter)\n",
    "        stats_down[0].append(self.loss)\n",
    "        \n",
    "        \n",
    "        for i in range(1,len(Y)):\n",
    "            \n",
    "            # update stats_up\n",
    "            omega_i=stats_up[i-1][0]*i/(i+1)+Y[i,0]/(1+self.lambd)/(i+1)\n",
    "            loss_i=((stats_up[i-1][1]+(1+self.lambd)*stats_up[i-1][0]**2)*i/(i+1)+Y[i,0]**2/(i+1))\\\n",
    "                   -(1+self.lambd)*(omega_i**2)\n",
    "            stats_up[i].append(omega_i)\n",
    "            stats_up[i].append(loss_i)\n",
    "\n",
    "\n",
    "            # update stats_down\n",
    "            omega_i_1=stats_down[i-1][0]*(N-i+1)/(N-i)-Y[i-1,0]/(1+self.lambd)/(N-i)\n",
    "            loss_i_1=((stats_down[i-1][1]+(1+self.lambd)*stats_down[i-1][0]**2)*(N-i+1)/(N-i)-Y[i-1,0]**2/(N-i))\\\n",
    "                   -(1+self.lambd)*(omega_i_1**2)\n",
    "            stats_down[i].append(omega_i_1)\n",
    "            stats_down[i].append(loss_i_1)\n",
    "            \n",
    "            # find out the best split point\n",
    "            if X[i,feature_i]!=X[i-1,feature_i] or i==len(Y):\n",
    "                if stats_up[i-1][-1]*i/N+stats_down[i][-1]*(N-i)/N<b_loss:\n",
    "                    b_split_left_stats=stats_up[i-1]\n",
    "                    b_split_right_stats=stats_down[i]\n",
    "                    b_split_left_size=i\n",
    "                    b_split_right_size=N-i\n",
    "                    \n",
    "                    b_loss=stats_up[i-1][-1]*i/N+stats_down[i][-1]*(N-i)/N\n",
    "                    b_split_value=X[i-1,feature_i]\n",
    "                   \n",
    "        return b_split_left_stats,b_split_right_stats,b_split_left_size,b_split_right_size,b_split_value,b_loss\n",
    "    \n",
    "    def batch_fit(self,X,Y,feature_i,feature_name_dtype=None):\n",
    "        '''calculate stats for every binary division of the XY into XY[:,feature_i]==value and XY[:,feature_i]!=value \n",
    "           for all possible values of feature_i\n",
    "           omega_batch,loss_batch: information about subsample XY[:,feature_i]==value\n",
    "           omega_ba_null,loss_ba_null: information about subsample XY[:,feature_i]!=value \n",
    "        '''\n",
    "        N=len(Y)\n",
    "        \n",
    "        b_split_left_stats=None\n",
    "        b_split_right_stats=None\n",
    "        b_split_left_size=1\n",
    "        b_split_right_size=N-1\n",
    "        \n",
    "        best_split_value=None\n",
    "        best_loss=np.Infinity\n",
    "        \n",
    "        # initialize stats_down\n",
    "        if not self.parameter:\n",
    "            self.fit(X,Y)\n",
    "        if not self.loss:\n",
    "            self.loss=self.loss_cal(X,Y)\n",
    "            \n",
    "        omega_tot=self.parameter\n",
    "        loss_tot=self.loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        #l,r: left bound and right bound of the batch\n",
    "        l=0\n",
    "        r=1\n",
    "        while r<len(Y):\n",
    "            while r<len(X) and X[r,feature_i]==X[r-1,feature_i]:\n",
    "                r+=1\n",
    "                \n",
    "            omega_batch=self.partial_fit(X[l:r],Y[l:r])\n",
    "            loss_batch=self.loss_cal(X[l:r],Y[l:r])\n",
    "                       \n",
    "            omega_ba_null=omega_tot*N/(N-(r-l))-np.sum(Y[l:r])/(1+self.lambd)/(N-(r-l))\n",
    "            loss_ba_null=((loss_tot+(1+self.lambd)*omega_tot**2)*N/(N-(r-l))-np.sum(Y[l:r]**2)/(N-(r-l)))\\\n",
    "                   -(1+self.lambd)*(omega_ba_null**2)\n",
    "            \n",
    "            \n",
    "            if loss_batch*(r-l)/N+loss_ba_null*(N-(r-l))/N<best_loss:\n",
    "                b_split_left_stats=[omega_batch,loss_batch]\n",
    "                b_split_right_stats=[omega_ba_null,loss_ba_null]\n",
    "                b_split_left_size=r-l\n",
    "                b_split_right_size=N-(r-l)\n",
    "                \n",
    "                best_loss=loss_batch*(r-l)/N+loss_ba_null*(N-(r-l))/N\n",
    "                best_split_value=X[r-1,feature_i]               \n",
    "            \n",
    "            l=r\n",
    "            r+=1\n",
    "            \n",
    "        return b_split_left_stats,b_split_right_stats,b_split_left_size,b_split_right_size,best_split_value,best_loss\n",
    "            \n",
    "                  \n",
    "    def partial_fit(self,X,Y,feature_name_dtype=None):\n",
    "        # conventional stochastic gradient method to train model\n",
    "        return np.mean(Y)/(1+self.lambd)\n",
    "    \n",
    "    def fit(self,X,Y,feature_name_dtype=None):\n",
    "        # conventional stochastic gradient method to train model\n",
    "        self.parameter=np.mean(Y)/(1+self.lambd)\n",
    "    \n",
    "    def predict(self,X,feature_name_dtype=None):\n",
    "        if len(np.shape(X))==2:\n",
    "            return np.array([self.parameter for i in range(X.shape[0])])        \n",
    "        elif len(np.shape(X))==1:\n",
    "            return np.array([self.parameter])\n",
    "        else:\n",
    "            raise ValueError('The shape of input data is incorrect')\n",
    "    \n",
    "    def loss_cal(self,X,Y,feature_name_dtype=None):\n",
    "        return np.mean(Y**2)-np.mean(Y)**2/(1+self.lambd)\n",
    "    \n",
    "    def model_description(self):\n",
    "        return 'predicted value is {}'.format(self.parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class linear_regression():\n",
    "    def __init__(self,hparameter=None,stats=None):\n",
    "        if hparameter is None:\n",
    "            self.lambd=0\n",
    "            self.sigma=0.05\n",
    "        else:\n",
    "            self.lambd=hparameter[0]\n",
    "            self.sigma=hparameter[1]\n",
    "\n",
    "        if stats is None:\n",
    "            self.Gamma=None\n",
    "            self.theta=None    \n",
    "            self.loss=None\n",
    "        else:           \n",
    "            self.Gamma=stats[0]\n",
    "            self.theta=stats[1]      \n",
    "            self.loss=stats[2]\n",
    "\n",
    "\n",
    "    \n",
    "    def online_fit(self,X,Y,feature_i,feature_name_dtype=None):\n",
    "        '''calculate stats for every binary division of the XY into XY[:i] and XY[i:] with i in range(len(XY))\n",
    "           stats_up: store information about subsample XY[:i]\n",
    "           stats_down: store information about subsample XY[i:]\n",
    "        '''\n",
    "        X_feature_i=X[:,feature_i]\n",
    "     \n",
    "        if not (feature_name_dtype is None):\n",
    "            X=X[:,feature_name_dtype[:,1]=='c']\n",
    "\n",
    "        N=len(Y)  \n",
    "        d=X.shape[1]+1\n",
    "        \n",
    "        b_split_value=None    \n",
    "        b_loss=np.Infinity  \n",
    "        b_split_left_size=1\n",
    "        b_split_right_size=N-1\n",
    "        \n",
    "        b_split_left_stats=None\n",
    "        b_split_right_stats=None\n",
    "               \n",
    "        \n",
    "        \n",
    "        # initialize stats_up\n",
    "        stats_up=[[] for i in range(len(Y))] \n",
    "        Gamma,theta_hat,M=self.partial_fit(X[:d],Y[:d])\n",
    "        # initialize the first d elements by the \n",
    "        for i in range(d):\n",
    "            stats_up[i].append(Gamma)\n",
    "            stats_up[i].append(theta_hat)\n",
    "            stats_up[i].append(M)                        # value of loss\n",
    "\n",
    "       \n",
    "        # initialize stats_down\n",
    "        if self.theta is None:\n",
    "            self.fit(X[d:],Y[d:])\n",
    "            \n",
    "        stats_down=[[] for i in range(len(Y))]\n",
    "        for i in range(d):\n",
    "            stats_down[i].append(self.Gamma)\n",
    "            stats_down[i].append(self.theta)\n",
    "            stats_down[i].append(self.loss)\n",
    "\n",
    "        \n",
    "        X=np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        \n",
    "        for i in range(d,X.shape[0]-d):         \n",
    "        #updata stats_up\n",
    "            # update Gamma matrix\n",
    "\n",
    "            half=np.tensordot(stats_up[i-1][0],X[i],axes=[1,0]) \n",
    "            numerator=np.tensordot(np.expand_dims(half,1),np.expand_dims(half,1),axes=[1,1])\n",
    "            \n",
    "            denominator=(1+np.tensordot(X[i],half,axes=[0,0]))          \n",
    "            Gammai=stats_up[i-1][0]-numerator/denominator\n",
    "            stats_up[i].append(Gammai)\n",
    "\n",
    "            misspred=np.tensordot(stats_up[i-1][1],X[i],axes=[0,0])-Y[i]\n",
    "\n",
    "            # update theta_hat   \n",
    "            theta_hati=stats_up[i-1][1]-np.tensordot(stats_up[i][0],X[i],axes=[1,0])*misspred\n",
    "            stats_up[i].append(theta_hati)\n",
    "\n",
    "\n",
    "            # update mean square error\n",
    "            Mi=i/(i+1)*stats_up[i-1][2]+misspred*misspred/(denominator*(i+1))    \n",
    "            stats_up[i].append(Mi)\n",
    "            \n",
    "         #updata stats_down   \n",
    "            # update Gamma matrix\n",
    "            half=np.tensordot(stats_down[i-1][0],X[i-1],axes=[1,0]) \n",
    "            numerator=np.tensordot(np.expand_dims(half,1),np.expand_dims(half,1),axes=[1,1])\n",
    "            denominator=(1-np.tensordot(X[i-1],half,axes=[0,0]))\n",
    "            Gammai=stats_down[i-1][0]+numerator/denominator\n",
    "            stats_down[i].append(Gammai)\n",
    "\n",
    "            misspred=np.tensordot(stats_down[i-1][1],X[i-1],axes=[0,0])-Y[i-1]\n",
    "\n",
    "            # update theta_hat   \n",
    "            theta_hati=stats_down[i-1][1]+np.tensordot(stats_down[i][0],X[i-1],axes=[1,0])*misspred\n",
    "            stats_down[i].append(theta_hati)\n",
    "\n",
    "\n",
    "            # update mean square error\n",
    "            Mi=(N-(i-1))/(N-i)*stats_down[i-1][2]-misspred*misspred/(denominator*(N-i))    \n",
    "            stats_down[i].append(Mi)\n",
    "            \n",
    "        # find out the best split point\n",
    "            if  X_feature_i[i]!=X_feature_i[i-1] or i==len(Y):\n",
    "                if stats_up[i-1][-1]*i/N+stats_down[i][-1]*(N-i)/N<b_loss:\n",
    "                    b_split_left_stats=stats_up[i-1]\n",
    "                    b_split_right_stats=stats_down[i]\n",
    "                    b_split_left_size=i\n",
    "                    b_split_right_size=N-i\n",
    "                    \n",
    "                    b_loss=stats_up[i-1][-1]*i/N+stats_down[i][-1]*(N-i)/N\n",
    "                    b_split_value=X_feature_i[i-1]\n",
    "                   \n",
    "        return b_split_left_stats,b_split_right_stats,b_split_left_size,b_split_right_size,b_split_value,b_loss\n",
    "        #return stats_up,stats_down\n",
    "    \n",
    "    def batch_fit(self,X,Y,feature_i,feature_name_dtype=None):\n",
    "        '''calculate stats for every binary division of the XY into XY[:,feature_i]==value and XY[:,feature_i]!=value \n",
    "           for all possible values of feature_i\n",
    "           omega_batch,loss_batch: information about subsample XY[:,feature_i]==value\n",
    "           omega_ba_null,loss_ba_null: information about subsample XY[:,feature_i]!=value \n",
    "        '''\n",
    "        X_feature_i=X[:,feature_i]\n",
    "        \n",
    "        if not (feature_name_dtype is None):\n",
    "            X=X[:,feature_name_dtype[:,1]=='c']                 \n",
    "       \n",
    "        N=len(Y)\n",
    "               \n",
    "        d=X.shape[1]+1\n",
    "        \n",
    "        b_split_left_stats=None\n",
    "        b_split_right_stats=None\n",
    "        b_split_left_size=1\n",
    "        b_split_right_size=N-1\n",
    "        \n",
    "        best_split_value=None\n",
    "        best_loss=np.Infinity\n",
    "        \n",
    "        # initialize stats_down\n",
    "\n",
    "        if self.theta is None:\n",
    "            self.fit(X[d:],Y[d:])\n",
    "\n",
    "        \n",
    "        X=np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "\n",
    "        stats_batch=[]\n",
    "        stats_ba_null=[]\n",
    "        \n",
    "        #l,r: left bound and right bound of the batch\n",
    "        l=0\n",
    "        r=1\n",
    "        while r<len(Y):\n",
    "            while r<len(X) and X_feature_i[r]==X_feature_i[r-1]:\n",
    "                r+=1\n",
    "                \n",
    "            stats_batch=self.partial_fit(X[l:r],Y[l:r])\n",
    "            stats_ba_null=self.partial_fit(np.concatenate(X[:l],X[r:],axis=0),np.concatenate(Y[:l],Y[r:],axis=0))\n",
    "                   \n",
    "            if stats_batch[-1]*(r-l)/N+stats_ba_null[-1]*(N-(r-l))/N<best_loss:\n",
    "                b_split_left_stats=stats_batch\n",
    "                b_split_right_stats=stats_ba_null\n",
    "                b_split_left_size=r-l\n",
    "                b_split_right_size=N-(r-l)\n",
    "                \n",
    "                best_loss=stats_batch[-1]*(r-l)/N+stats_ba_null[-1]*(N-(r-l))/N\n",
    "                best_split_value=X_feature_i[r-1]               \n",
    "            \n",
    "            l=r\n",
    "            r+=1\n",
    "            \n",
    "        return b_split_left_stats,b_split_right_stats,b_split_left_size,b_split_right_size,best_split_value,best_loss\n",
    "            \n",
    "                  \n",
    "    def partial_fit(self,X,Y,feature_name_dtype=None): \n",
    "        if not (feature_name_dtype is None):\n",
    "            X=np.concatenate((X[:,feature_name_dtype[:,1]=='c'],np.ones((X.shape[0],1))),axis=1)\n",
    "        else:\n",
    "            X=np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "            \n",
    "        N=len(Y)    \n",
    "        a=np.zeros([X.shape[1],X.shape[1]])\n",
    "        b=np.zeros([X.shape[1]])\n",
    "        c=np.zeros([1])\n",
    "        for i in range(N):\n",
    "            a+=np.tensordot(np.expand_dims(X[i],1),np.expand_dims(X[i],1),axes=[1,1])\n",
    "            b+=X[i]*Y[i]\n",
    "            c+=Y[i]*Y[i]\n",
    "\n",
    "        Gamma=np.linalg.inv(a+self.sigma*np.identity(X.shape[1]))\n",
    "        # use normal equation to calculate theta\n",
    "        theta_hat=np.tensordot(Gamma,b,axes=[1,0])\n",
    "        # use normal equation to calculate Mean Square Error\n",
    "        M=(-np.tensordot(np.tensordot(b,Gamma,axes=[0,0]),b,axes=[0,0])+c)/N\n",
    " \n",
    "        return [Gamma,theta_hat,M]\n",
    "    \n",
    "    def fit(self,X,Y,feature_name_dtype=None):    \n",
    "        if not (feature_name_dtype is None):\n",
    "            X=np.concatenate((X[:,feature_name_dtype[:,1]=='c'],np.ones((X.shape[0],1))),axis=1)\n",
    "        else:\n",
    "            X=np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "            \n",
    "        a=np.zeros([X.shape[1],X.shape[1]])\n",
    "        b=np.zeros([X.shape[1]])\n",
    "        c=np.zeros([1])\n",
    "        for i in range(X.shape[0]):\n",
    "            a+=np.tensordot(np.expand_dims(X[i],1),np.expand_dims(X[i],1),axes=[1,1])\n",
    "            b+=X[i]*Y[i]\n",
    "            c+=Y[i]*Y[i]\n",
    "\n",
    "        Gamma=np.linalg.inv(a+self.sigma*np.identity(X.shape[1]))\n",
    "        # use normal equation to calculate theta\n",
    "        theta_hat=np.tensordot(Gamma,b,axes=[1,0])\n",
    "        # use normal equation to calculate Mean Square Error\n",
    "        M=(-np.tensordot(np.tensordot(b,Gamma,axes=[0,0]),b,axes=[0,0])+c)/X.shape[0]\n",
    "        \n",
    "        self.Gamma=Gamma\n",
    "        self.theta=theta_hat\n",
    "        self.loss=M\n",
    "\n",
    "    \n",
    "    \n",
    "    def predict(self,X,feature_name_dtype=None):\n",
    "          \n",
    "        if len(np.shape(X))==2:\n",
    "            if not (feature_name_dtype is None):\n",
    "                X=np.concatenate((X[:,feature_name_dtype[:,1]=='c'],np.ones((X.shape[0],1))),axis=1)\n",
    "            else:\n",
    "                X=np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "\n",
    "            return np.tensordot(self.theta,X,axes=[0,1])   \n",
    "        \n",
    "        elif len(np.shape(X))==1:\n",
    "            if not (feature_name_dtype is None):\n",
    "                X=np.concatenate((X[feature_name_dtype[:,1]=='c'],[1]),axis=0)\n",
    "            else:\n",
    "                X=np.concatenate((X,[1]),axis=0)               \n",
    "            return np.array([np.tensordot(self.theta,X,axes=[0,0])])\n",
    "        else:\n",
    "            raise ValueError('The shape of input data is incorrect')\n",
    "    def model_description(self):\n",
    "        model_eq='y='\n",
    "        for i in range(len(self.theta)-1):\n",
    "            model_eq+='+{}*X_{}'.format(round(self.theta[i],5),i) if self.theta[i]>=0 \\\n",
    "                      else '{}*X_{}'.format(round(self.theta[i]),i)\n",
    "        model_eq+='{}'.format(self.theta[-1])\n",
    "        return model_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def xgboost_tree_decorator(decision_tree,max_num_trees=10):\n",
    "    class xgboost_tree():\n",
    "        \"\"\"\n",
    "        X=np.array(shape=[samplesize,feature])\n",
    "        Y=np.array(shape=[samplesize])\n",
    "        feature_name_type=np.array([(name,'c'/'d')...])\n",
    "        where 'c' respresents continuous variabl and 'd' represents discrete variable\n",
    "        \"\"\"\n",
    "        def __init__(self,max_num_trees=max_num_trees):\n",
    "            self.max_num_trees=max_num_trees          \n",
    "\n",
    "\n",
    "        def fit(self,X,Y,feature_name_dtype):\n",
    "\n",
    "            # just to store the root of the tree \n",
    "            # consistent check whether the shape of inputs are expected:\n",
    "            condition1= ((len(np.shape(X))==2)\n",
    "                        and (len(np.shape(feature_name_dtype))==2))         \n",
    "            condition2= ((np.shape(X)[0]==np.shape(Y)[0]) \n",
    "                         and (np.shape(X)[1]==np.shape(feature_name_dtype)[0]))\n",
    "\n",
    "            if condition1 and condition2:\n",
    "                if len(np.shape(Y))==1: \n",
    "                    Y = np.expand_dims(Y, axis=1)\n",
    "\n",
    "                # Add Y,g,h as last columns to X so easier to split them together\n",
    "                XY = np.concatenate((X, Y), axis=1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('The shapes of inputs are not compatible')\n",
    "\n",
    "            # store data \n",
    "            self.XY_mul=np.array([sorted(XY,key=lambda unit: unit[i]) for i in range(X.shape[1])])\n",
    "            self.trees=[]\n",
    "            self.feature_name_dtype=feature_name_dtype\n",
    "            \n",
    "        \n",
    "                  \n",
    "            for i in range(self.max_num_trees):\n",
    "                tree_i=decision_tree()\n",
    "                tree_i.gamma=tree_i.gamma/(i+1)\n",
    "                tree_i.fit(XY_mul=self.XY_mul,feature_name_dtype=feature_name_dtype)\n",
    "                self.update_Y(tree_i.root)\n",
    "                self.trees.append(tree_i)\n",
    "                                        \n",
    "            # release memory\n",
    "            self.XY_mul=0\n",
    "\n",
    "\n",
    "        def predict(self,X):\n",
    "            result=np.zeros((len(X)))\n",
    "            for tree_i in self.trees:\n",
    "                result+=tree_i.predict(X)\n",
    "                \n",
    "            return result \n",
    "        \n",
    "        def plot_tree(self):\n",
    "            dot = Digraph(comment='XGboost Trees')\n",
    "            for tree_i in self.trees:\n",
    "                dot.subgraph(tree_i.export_graphviz())\n",
    "            dot.render(view=True)\n",
    "        \n",
    "        def update_Y(self,node):\n",
    "            if node.true is None:\n",
    "                y_hat=node.predict(self.XY_mul[node.sample_index][:,:-1])     \n",
    "                y_hat=np.expand_dims(y_hat,axis=1)\n",
    "                zeros=np.zeros((len(y_hat),len(self.feature_name_dtype)))\n",
    "                y_hat=np.concatenate((zeros,y_hat),axis=1)      \n",
    "                self.XY_mul[node.sample_index]-=y_hat\n",
    "                node.sampel_index=None\n",
    "            else:\n",
    "                node.sampel_index=None\n",
    "                self.update(node.true)\n",
    "                self.update(node.false)\n",
    "                      \n",
    "    return xgboost_tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
