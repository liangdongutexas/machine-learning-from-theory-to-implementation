{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for Bayes rule in a sample is the following:\n",
    "\\begin{align}\n",
    "p(\\{Y\\}|\\{X\\})=\t\\frac{p(\\{X\\}|\\{Y\\})p(\\{Y\\})}{p(\\{X\\})},\n",
    "\\end{align}\n",
    "where $\\{X\\}$ denotes a sample of random variables and $X$ denotes a single unit in the sample.\n",
    "Because the unit in a sample is assumed to be iid, given the realziation of the sample $\\{x,y\\}$, the conditional probability of obeserving the predictor-outcome pair in the current sample is \n",
    "\\begin{align}\n",
    "p(\\{y\\}|\\{x\\})\\propto\\prod_{i}p(x^{i}|y^{i})p(y^{i}),\n",
    "\\end{align}\n",
    "where $i$ is the label for units in the sample.\n",
    "\n",
    "Assumptions are made about individual likelihood $p(X|Y)$ with some unkonwn parameter $\\theta$, e.g. gaussian distribution with standard deviation and mean as the parameter. Then some estimation method is used to fix the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP in Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method used in sklearn is MAP. It is to maximize the posteriori:\n",
    "\\begin{align}\n",
    "\\theta=&\t\\arg\\max_{\\theta}\\log p(\\{y\\}|\\{x\\})\\\\\n",
    "=&\t\\arg\\max_{\\theta}\\log\\prod_{i}p(x^{i}|y^{i})p(y^{i})\\\\\n",
    "=&\t\\arg\\max_{\\theta}\\sum_{i}[\\log p(x^{i}|y^{i})+\\log p(y^{i})].\n",
    "\\end{align}\n",
    "When the mariginal distribution $p(Y)$ does not contains the parameter in the likelihood function, $p(Y)$ can be estimated by the sample distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.naive_bayes as NB\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data to pandas format\n",
    "iris_data=pd.read_csv('Iris.csv')\n",
    "# drop the id column to make the hist paragraph look better\n",
    "iris_data=iris_data.drop(columns='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, y_p = iris_data.iloc[:,:4],iris_data.iloc[:,[4]]\n",
    "X=X_p.to_numpy()\n",
    "y=y_p.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "y_train=y_train.reshape(y_train.shape[0],)\n",
    "y_test=y_test.reshape(y_test.shape[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption of likelihood function reads:\n",
    "\\begin{align}\n",
    "p(X|Y)=\\frac{1}{\\sqrt{(2\\pi)^{d}|\\Sigma_{Y}|}}\\exp(-\\frac{1}{2}(X-\\mu_{Y})_{i}\\Sigma_{Y}^{-1ij}(X-\\mu_{Y})_{j}).\n",
    "\\end{align}\n",
    "There is no need to apply numerical method e.g. SGD to calculate the covariance matrix $\\Sigma$ and mean $\\mu$ as can be proved by hand they simply given by the sample covariance matrix and sample mean for each class of $Y$.\n",
    "\n",
    "Given $X_{test}$, if we only want to achieve the prediction and not care about the probability distributio on each class, we can iterate among all classes to find the largest value of \n",
    "\\begin{align}\n",
    "p(X_{test}|Y)p(Y)=&\\frac{1}{\\sqrt{|\\Sigma_{Y}|}}\\exp(-\\frac{1}{2}(X_{test}-\\mu_{Y})_{i}\\Sigma_{Y}^{-1ij}(X_{test}-\\mu_{Y})_{j})p(Y).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the GaussianBayes code written by myself. I didn't find a GaussianBayes package\n",
    "class GaussianBayes():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,Y):\n",
    "        data=pd.DataFrame(data=X)\n",
    "        data['class']=Y\n",
    "        self.sigmas=[]\n",
    "        self.mus=[]\n",
    "        self.classes=data.iloc[:,-1].unique()\n",
    "        self.p_y=data.iloc[:,-1].value_counts()\n",
    "        \n",
    "        for y in self.classes:\n",
    "            # calculate covariance matrix for each class\n",
    "            self.sigmas.append(data.loc[data.iloc[:,-1]==y].corr())\n",
    "            # calculate mean vector for each class\n",
    "            self.mus.append(data.loc[data.iloc[:,-1]==y].mean())\n",
    "    def predict(self,X):\n",
    "        # sigmas.shape=(#_class,#_predictor,#_predictor)\n",
    "        # mus.shape=(#_class,#_predictor)\n",
    "        Y=[]\n",
    "        p_y=np.array([self.p_y[y] for y in self.classes])\n",
    "        sigmas=np.array([i.to_numpy() for i in self.sigmas])\n",
    "        mus=np.array([i.to_numpy() for i in self.mus])  \n",
    "        for x in X:\n",
    "            # power.shape=(#class,1,1)\n",
    "            power=-np.matmul(np.subtract(mus,x)[:,np.newaxis,:],np.matmul(np.linalg.inv(sigmas),np.subtract(mus,x)[:,:,np.newaxis]))             \n",
    "            power=power.reshape(power.shape[0])\n",
    "            p_xy=np.multiply(np.multiply(np.reciprocal(np.sqrt(np.linalg.det(sigmas))),np.exp(power)),p_y)\n",
    "            Y.append(self.classes[np.argmax(p_xy)])\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 45 points : 3\n"
     ]
    }
   ],
   "source": [
    "GB=GaussianBayes()\n",
    "GB.fit(X_train, y_train)\n",
    "y_pred = GB.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "     % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes is based on the conditional independence assumption between features such that\n",
    "\\begin{align}\n",
    "p(x_{j}^{i}|y^{i})=\t\\prod_{j}p(x_{j}^{i}|y^{i}),\n",
    "\\end{align}\n",
    "where j is the label of feature. So MAP in the case of naive Bayes can be further written as:\n",
    "\\begin{align}\n",
    "\\theta=\t\\arg\\max_{\\theta}\\sum_{i}[\\sum_{j}\\log p(x_{j}^{i}|y^{i})+\\log p(y^{i})].\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes\n",
    "\\begin{align}\n",
    "p(x_{j}|y)=\\frac{1}{\\sqrt{2\\pi\\sigma_{y}}}\\exp(-\\frac{(x_{j}-\\mu_{y})^{2}}{2\\sigma_{y}^{2}}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 45 points : 0\n"
     ]
    }
   ],
   "source": [
    "gnb = NB.GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "     % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 45 points : 18\n"
     ]
    }
   ],
   "source": [
    "mnb = NB.MultinomialNB()\n",
    "y_pred = mnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "     % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 45 points : 4\n"
     ]
    }
   ],
   "source": [
    "mnb = NB.CategoricalNB()\n",
    "y_pred = mnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "     % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
