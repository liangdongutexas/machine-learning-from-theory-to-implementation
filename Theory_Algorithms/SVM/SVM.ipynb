{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance beween a point and hyperplane\n",
    "In N dimensional space, the minimal distance from a point $\\{y_{i}\\}$ to the N-1 hyperplane $w_{i}x_{i}+b=0$ can be achieved by the minimal of the following Lagrange function:\n",
    "\\begin{align}\n",
    "L=\t[(x_{i}-y_{i})(x_{i}-y_{i})+\\mu(w_{i}x_{i}+b)],\n",
    "\\end{align}\n",
    "which is given by\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial x_{j}}=\t[2(x_{j}-y_{j})+\\mu w_{j}]=0.\n",
    "\\end{align}\n",
    "Substituting $x_{i}=y_{i}-\\frac{1}{2}\\mu w_{i}$ back to the hyperplane equation, we have\n",
    "\\begin{align}\n",
    "0=&\tw_{i}(y_{i}-\\frac{1}{2}\\mu w_{i})+b,\\\\\n",
    "\\mu=&\t\\frac{2(b+w_{i}y_{i})}{|w|^{2}}.\n",
    "\\end{align}\n",
    "Then the minimal distance between y and hyperplane reads:\n",
    "\\begin{align}\n",
    "d=\\frac{1}{2}|\\mu||w|=\t\\frac{|b+w_{i}y_{i}|}{|w|}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "Suppose we have a sample with predictors $\\{X^{i}\\}$ and outcome $\\{Y^{i}\\}$. Suppose the outcome takes two values $\\pm1$, given feature $x$, the outcome is predicted according to \n",
    "\\begin{align}\n",
    "y=\tsign(b+w^{T}x)1.\n",
    "\\end{align}\n",
    "If the outcome is correctly predicted, the distance can also be written as:\n",
    "\\begin{align}\n",
    "d=\t\\frac{y(b+w^{T}x)}{|w|},\n",
    "\\end{align}\n",
    "with the normal direction of the hyperplane pointing to the Y=+1 class. \n",
    "\n",
    "The goal of SVM is to maximize the smallest distance among all data points $d_{min}$ by varying $\\{w,b\\}$. $d_{min}$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "d_{min}=\t\\arg\\min_{i}\\{d^{i}\\},\n",
    "\\end{align}\n",
    "\n",
    "with $d^{i}=\\frac{y^{i}(b+w^{T}x^{i})}{|w|}$. We denote the observation corresponding to $d_{min}$ as $x_{min}$, then an alternative way to formulate this maximazation process is:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg\\max_{\\{w,b\\}}&\t\\frac{y_{min}(b+w^{T}x_{min})}{|w|},\\\\\n",
    "s.t.&\t\\frac{y^{i}(b+w^{T}x^{i})}{|w|}-\\frac{y_{min}(b+w^{T}x_{min})}{|w|}\\geqslant0.\n",
    "\\end{align}\n",
    "\n",
    "If the data point corresponds to the minimal distance is unchanged during the variation process, we can utilize the scale invariance of $\\{w,b\\}$ in representing the same hyperplane to make $y_{min}(b+w^{T}x_{min})=1$. Then the problem can be reexpressed as:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg\\min_{\\{w,b\\}}&\t|w|^{2},\\\\\n",
    "s.t.&\ty^{i}(b+w^{T}x^{i})-1\\geqslant0\\\\\n",
    "&\ty_{min}(b+w^{T}x_{min})=1.\n",
    "\\end{align}\n",
    "\n",
    "The problem involves inequality constraints and can be solved through the Karush–Kuhn–Tucker conditions, which will be introduced in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrange multiplier\n",
    "<img src=\"LagrangeMultipliers2D.svg.png\" width=\"50%\" height=\"50%\">\n",
    "The Lagrange multiplier method is to solve the following problem:\n",
    "\\begin{align}\n",
    "maximize:&\tf(x)\\\\\n",
    "subject\\ to:&\tg(x)=0.\n",
    "\\end{align}\n",
    "In the two-dimensional example shown by the above picture, we need to find the maximum of $f(x,y)$ on the red line of condition $g(x,y)=0$. A necessary condition is that the derivative of $f(x,y)$ along the tangent direction of the red line is zero. This condition happens in two cases: (1) $\\nabla f=0$ in regardless of g, (2) $\\nabla f$ parallel to $\\nabla g$. The two cases can be denoted by a single expression:\n",
    "\\begin{align}\n",
    "\\nabla f=\t\\lambda\\nabla g,\n",
    "\\end{align}\n",
    "where $\\lambda$ is called the Lagrange multiplier and equals to zero for the first case. Of course the above equation has to be combined with the feasibility condtion\n",
    "\\begin{align}\n",
    "g(x)=\t0.\n",
    "\\end{align}\n",
    "Then the two equations can be further combined as the stationary points condition of the Lagrangian $\\mathcal{L}(x,\\lambda)=f(x)+\\lambda g(x)$:\n",
    "\\begin{align}\n",
    "\\nabla_{x,\\lambda}\\mathcal{L}=\t0,\n",
    "\\end{align}\n",
    "where $\\mathcal{L}(x,\\lambda)$ is a function depending on extra dimensions denoted by $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karush–Kuhn–Tucker conditions\n",
    "<img src=\"Inequality_constraint_diagram.svg.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "KKT conditions are generalization of the Lagrange condition to include inequality constraints:\n",
    "\\begin{align}\n",
    "maximize:&\tf(x),\\\\\n",
    "subject\\ to:&\tg(x)\\leqslant0\\\\\n",
    "\t&h(x)=0.\n",
    "\\end{align}\n",
    "The idea is based on a simple observation that if the maximum happens on the boundary of $g(x)=0$, the problem is reduced to the Lagrange problem with additional constraints, else (happens in the domain $g(x)<0$) the inequality condition can actually be discarded and the problem is reduced to the Lagrange case only with constraint h. Following the Lagrange case, we define the Lagrangian as\n",
    "\\begin{align}\n",
    "\\mathcal{L}=\tf(x)+\\mu g(x)+\\lambda h(x),\n",
    "\\end{align}\n",
    "the two cases can be denoted by the complementary slackness condition:\n",
    "\\begin{align}\n",
    "\\mu g(x)=\t0.\n",
    "\\end{align}\n",
    "Of course the primal feasibility condion:\n",
    "\\begin{align}\n",
    "g(x)\\leqslant\t0,\n",
    "\\end{align}\n",
    "and stationary condition:\n",
    "\\begin{align}\n",
    "\\nabla_{x,\\lambda}\\mathcal{L}=\t0,\n",
    "\\end{align}\n",
    "should be satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()\n",
    "x=iris['data'][:,(2,3)] # petal length, petal width\n",
    "y=(iris['target']==2).astype(np.float64) # Iris virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf=Pipeline([('scalar',StandardScaler()),('linear_svc',LinearSVC(C=1,loss='hinge'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(x,y)\n",
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
