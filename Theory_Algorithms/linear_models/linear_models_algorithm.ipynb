{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Here, I will implement the logistic regression with tensorflow. Tensorflow here serves as a tool to conduct gradient descent to find the parameter $\\theta$ that minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow is used to conduct stochastic gradient descent to calculate the parameters $\\beta$. The tricky part is the calculation of loss function.\n",
    "\n",
    "1. one hot encoding the outcome, so Y  becomes \\[\\[1,0,0,...\\],...\\[0,...1,...,0\\],... \\]\n",
    "2. use softmax function to calculate $\\hat{p}(y|x)$ and achieve a prediction in the form \\[...\\[$\\hat{p}_0(y|x)$,$\\hat{p}_1(y|x)$,...\\],...\\]\n",
    "3. the loss function then is the average of the dot product of each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf   \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_action():\n",
    "    \n",
    "    ## uses tensorflow to do backpropagation onece for each epoch.\n",
    "    def train_one_step(self,X,Y):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predict = self.predict(X)\n",
    "            loss=self.loss(predict, Y)\n",
    "            optimizer=self.optimizer\n",
    "            # compute gradient\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "            # update to weights\n",
    "            optimizer.apply_gradients(zip(grads, self.trainable_variables))   \n",
    "        \n",
    "    \n",
    "    def save_model(self,filename):\n",
    "        stored_variables=np.array([i.numpy() for i in self.trainable_variables])\n",
    "        np.save(filename, stored_variables,allow_pickle=True, fix_imports=True)\n",
    "        print('model has been saved')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shape is a list of tuples.\n",
    "## The i_th element represents the tensor structure of the i_th layer.\n",
    "## The 0_th element represents the shape of the input.\n",
    "## The -1_th element represents the shape of the output.\n",
    "class model(train_action):\n",
    "    \n",
    "    def __init__(self,optimizer=tf.keras.optimizers.SGD(lr=0.1)):       \n",
    "        super().__init__()\n",
    "        ## the parameters in this model\n",
    "        self.optimizer=optimizer\n",
    "                           \n",
    "    def fit(self,X,Y,epochs=10,regre=False,save_name=None):\n",
    "            \n",
    "        condition1= (np.shape(X)[0]==np.shape(Y)[0])\n",
    "        \n",
    "        #whether it is a regression problem or classification problem\n",
    "        if condition1:\n",
    "            if len(np.shape(Y))==1:\n",
    "                Y=np.expand_dims(Y,axis=1)      \n",
    "        else:\n",
    "            raise ValueError('shape does not conforms to \\(sample,feature \\)')    \n",
    "        \n",
    "        self.encoder=OneHotEncoder(handle_unknown='ignore') # one hot encoding Y for the calculation convenience of crossentropy\n",
    "        Y_onehot=self.encoder.fit_transform(Y).toarray()\n",
    "        \n",
    "        \n",
    "        self.regre=regre  \n",
    "        self.beta=tf.Variable(tf.random.truncated_normal(shape=(np.shape(X)[1]+1,np.shape(Y_onehot)[1])))\n",
    "        self.trainable_variables=[self.beta]\n",
    "        \n",
    "        \n",
    "        X=tf.constant(X,dtype=tf.float32)\n",
    "        #concatenate ones to the second dimension of X\n",
    "        X=tf.concat([X,tf.ones(shape=(X.shape[0],1),dtype=tf.float32)],axis=1)\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs): \n",
    "            predict=self.predict(X)\n",
    "            print('for epoch {}, loss is {}'.format(epoch,self.loss(predict,Y_onehot)))\n",
    "            self.train_one_step(X,Y_onehot)  \n",
    "        if save_name:\n",
    "            self.save_model(save_name) \n",
    "                                \n",
    "        \n",
    "                           \n",
    "            \n",
    "    def predict(self,X=None):\n",
    "        \"\"\"The structure of this model: a flow of how output Y is generated \n",
    "        given the input X. The output is returned in the end.\"\"\"\n",
    "        \n",
    "        predict=tf.linalg.matmul(X,self.beta)\n",
    "        \n",
    "             \n",
    "        return predict\n",
    "    \n",
    "    \n",
    "    def loss(self,predict,Y_onehot):\n",
    "        \"\"\"The loss function to be minimized with respect to the trainable_variables.\n",
    "        In general, it is a function of the prediction and the real data Y.\n",
    "        The result should be returned\"\"\"\n",
    "        \n",
    "        if self.regre:  # regression problem\n",
    "            loss=tf.math.reduce_mean(tf.keras.losses.MSE(predict,Y))\n",
    "       \n",
    "        else: # classification problem\n",
    "            loss=tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(Y_onehot, predict, axis=-1, name=None))\n",
    "        return loss\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "X=iris.data[:,2:] # petal length and width\n",
    "Y=iris.target\n",
    "feature_names=iris.feature_names[2:]\n",
    "feature_name_type=[[i,'c'] for i in feature_names]\n",
    "Y=np.expand_dims(Y,axis=1)\n",
    "Y_onehot=OneHotEncoder(handle_unknown='ignore').fit_transform(Y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 0, loss is 9.255451202392578\n",
      "for epoch 1, loss is 7.4199090003967285\n",
      "for epoch 2, loss is 5.625100612640381\n",
      "for epoch 3, loss is 3.914612293243408\n",
      "for epoch 4, loss is 2.439427375793457\n",
      "for epoch 5, loss is 1.6804108619689941\n",
      "for epoch 6, loss is 1.4050358533859253\n",
      "for epoch 7, loss is 1.239380955696106\n",
      "for epoch 8, loss is 1.1745182275772095\n",
      "for epoch 9, loss is 1.1530745029449463\n"
     ]
    }
   ],
   "source": [
    "testmodel=model()\n",
    "testmodel.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[-0.31379506, -0.3368296 , -0.3491938 ],\n",
       "        [ 0.36098498,  1.3118964 ,  0.6221312 ],\n",
       "        [-0.22255819, -0.7897023 ,  0.6636022 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [['Male'], ['Female'], ['Tran'],['Les']]\n",
    "enc.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([['Female', 1], ['Male', 2]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]] \n",
    "labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]] \n",
    "tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
