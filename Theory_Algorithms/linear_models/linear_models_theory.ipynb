{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Table of contents:\n",
    "    * [Linear regression](#linear)\n",
    "    * [Logistic regression](#logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression  <a name=\"linear\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model of linear regression assumed a relation between the outcome and independent variables as \n",
    "$$\n",
    "\\hat{y}=\\boldsymbol{\\theta}\\cdot \\boldsymbol{x}+\\epsilon,\n",
    "$$\n",
    "where $\\epsilon$ is the noise term and $\\boldsymbol{\\theta}$ is the linear coefficient.\n",
    "\n",
    "The essential difference between frequentist view and Bayesian view is whether the coefficient $\\boldsymbol{\\theta}$ is treated as a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frequentist perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function can be given by the mean square error as:\n",
    "$$\n",
    "MSE=\\frac{1}{N}\\sum_{i}(\\theta_{j} x_{i}^{j}-y_i)^2.\n",
    "$$\n",
    "We can see that in terms of the model parameter $\\boldsymbol{\\theta}$, the MSE is always a second order polynomial function with the form:\n",
    "$$\n",
    "MSE=\\frac{1}{N}[a^{mn}\\theta_{m}\\theta_{n}-2 b^{k}\\theta_{k}+c],\n",
    "$$\n",
    "with the parameters $a^{mn},b^{k},c$ given by\n",
    "\\begin{align}\n",
    "a^{mn}=& \\sum_i x_{i}^{m} x_{i}^{n},\\\\\n",
    "b^{k}=&\\sum_i x_{i}^{k} y_{i},\\\\\n",
    "c=&\\sum_i y_{i} y_{i}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation**\n",
    "\n",
    "The analytic result for the least MSE reads:\n",
    "\\begin{align} \n",
    "\\hat{\\theta}=\\boldsymbol{a}^{-1}\\boldsymbol{b}.\n",
    "\\end{align}\n",
    "\n",
    "And the minimal of the MSE is given by\n",
    "\\begin{align}\n",
    "\\frac{1}{N}(\\hat{\\boldsymbol{\\theta}}^{T} \\cdot \\boldsymbol{a} \\cdot \\hat{\\boldsymbol{\\theta}}-2\\boldsymbol{b}^T \\cdot \\hat{\\boldsymbol{\\theta}} +c) =& \\frac{1}{N}[(\\boldsymbol{a}^{-1}\\boldsymbol{b})^{T}\\boldsymbol{a}(\\boldsymbol{a}^{-1}\\boldsymbol{b})-2\\boldsymbol{b}^T(\\boldsymbol{a}^{-1}\\boldsymbol{b})+c] \\\\\n",
    "=&\\frac{1}{N}(-\\boldsymbol{b}^T\\boldsymbol{a}^{-1}\\boldsymbol{b}+c)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Complexity\n",
    "If we use normal equation to compute the optimal solution, calculating the matrix $xx^{T}$ needs $O(nd^{2})$ with $n$ the sample size and $d$ the feature dimension.  The computational complexity of inverting the matrix is typically about $O(d^{2.4})$ to $O(d^{3})$ (depending on the implementation). So it takes $O(nd^{2}+d^{3})$ to get the inverse matrix. \n",
    "\n",
    "Then the calculation of $\\hat{\\theta}$ takes $O(d^2)$ and MSE takes $O(d^2+d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model of linear regression assumed a relation between the outcome and independent variables as \n",
    "$$\n",
    "y=\\boldsymbol{\\theta}\\cdot \\boldsymbol{x}+\\epsilon,\n",
    "$$\n",
    "where $\\epsilon$ is the noise term and $\\boldsymbol{\\theta}$ is the linear coefficient.\n",
    "\n",
    "The essential difference between frequentist view and Bayesian view is whether the coefficient $\\boldsymbol{\\theta}$ is treated as a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, Bayesian methods starts from a proper factorization of the total joint distribution of both hidden variables and observables. In the example of linear regression:\n",
    "$$\n",
    "p(y,x,\\theta,\\sigma^2)=p(y|x,\\theta,\\sigma^2)p(x|\\theta,\\sigma^2)p(\\theta|\\sigma^2)p(\\sigma^2).\n",
    "$$\n",
    "Due to the independence assumed between variables, we have\n",
    "$$\n",
    "p(y,x,\\theta,\\sigma^2)=p(y|x,\\theta,\\sigma^2)p(x)p(\\theta|\\sigma^2)p(\\sigma^2).\n",
    "$$\n",
    "Then to get the conditional probability  $p(\\theta,\\sigma^2|y,x)$, we can either substitute the observed value of $x,y$ and conduct MCMC with respect to the variable $\\theta,\\sigma^2$ to the above expression or if possible analytically integrate out $\\theta,\\sigma^2$ and divide the above expression by the marginal distribution $p(x,y)$.\n",
    "\n",
    "The analytic method although not always feasible, however, can expose some properties of $p(\\theta,\\sigma^2|y,x)$. For this example, we have\n",
    "\\begin{align}\n",
    "p(y,x)=&\\int d\\theta d\\sigma^2p(y|x,\\theta,\\sigma^2)p(x)p(\\theta|\\sigma^2)p(\\sigma^2)\\\\\n",
    "      =& p(x)\\int d\\theta d\\sigma^2p(y|x,\\theta,\\sigma^2)p(\\theta|\\sigma^2)p(\\sigma^2).\n",
    "\\end{align}\n",
    "Thus we can see that when $p(y,x,\\theta,\\sigma^2)$ divided by $p(y,x)$, the factor $p(x)$ is cancelled thus we conclude that\n",
    "$$\n",
    "p(\\theta,\\sigma^2|y,x)\\propto p(y|x,\\theta,\\sigma^2)p(\\theta|\\sigma^2)p(\\sigma^2),\n",
    "$$\n",
    "where the distribution $p(x)$ can be neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression model, it is assumed that\n",
    "$$\n",
    "p(y|x,\\theta,\\sigma^2)=\\frac{1}{\\sigma \\sqrt{2 \\pi}}\\exp[-\\frac{(y-\\theta x)^2}{2\\sigma^2}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression <a name=\"logistic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary case, the probability of belonging to the first class given predictors $X$ reads:\n",
    "\\begin{align}\n",
    "\\hat{p}=h_{\\theta}(X)=\\sigma(\\theta^T \\cdot X)\n",
    "\\end{align}\n",
    "where the sigmoid function reads: $\\sigma(t)=\\frac{1}{1+\\exp(-t)} $.\n",
    "Cost function:\n",
    "\\begin{align}\n",
    "J(\\theta)=&-\\frac{1}{m}\\sum_{i}[y_{i}\\ln(\\hat{p}_i)+(1-y_{i})\\ln(1-\\hat{p}_i)]\\\\\n",
    "\\hat{p}_{i}=&\\frac{1}{1+\\exp(-\\theta_{l}x_{i}^{l})}.\n",
    "\\end{align}\n",
    "\n",
    "The derivative of cost function with respect to $\\theta$ reads:\n",
    "\\begin{align}\n",
    "\\partial_{\\theta_{j}}J(\\theta)=&-\\frac{1}{m}\\sum_{i}[y_{i}\\frac{1}{\\hat{p}_{i}}\\partial_{\\theta_{j}}\\hat{p}_{i}+(1-y_{i})\\frac{1}{1-\\hat{p}_{i}}(-\\partial_{\\theta_{j}}\\hat{p}_{i})].\n",
    "\\end{align}\n",
    "\n",
    "the derivative of sigmoid function is\n",
    "\\begin{align}\n",
    "\\partial_{t}\\sigma(t)=&\\frac{\\exp(-t)}{(1+\\exp(-t))^{2}}\\\\\n",
    "=&\\sigma(t)(1-\\sigma(t))\n",
    "\\end{align}\n",
    "\n",
    "so the above equation reads:\n",
    "\\begin{align}\n",
    "\\partial_{\\theta_{j}}J(\\theta)=&-\\frac{1}{m}\\sum_{i}[y_{i}\\frac{1}{\\hat{p}_{i}}-(1-y_{i})\\frac{1}{1-\\hat{p}_{i}}]\\partial_{\\theta_{j}}\\hat{p}_{i}\\\\\n",
    "=&-\\frac{1}{m}\\sum_{i}[y_{i}\\frac{1}{\\hat{p}_{i}}-(1-y_{i})\\frac{1}{1-\\hat{p}_{i}}]\\hat{p}_{i}(1-\\hat{p}_{i})x_{i}^{j}\\\\\n",
    "=&-\\frac{1}{m}\\sum_{i}[y_{i}(1-\\hat{p}_{i})-(1-y_{i})\\hat{p}_{i}]x_{i}^{j}\\\\\n",
    "=&\\frac{1}{m}\\sum_{i}[\\hat{p}_{i}-y_{i}]x_{i}^{j}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression: general cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model is a linear discriminative model directly simulating the conditioinal probability $p(y|x)$. With $M$ possible outcomes, the probability is given by the formula:\n",
    "\n",
    "\\begin{align}\n",
    "z_i(x)&=\\beta_{ij}\\phi_{j}(x)+\\beta_{0},\\\\\n",
    "\\hat{p}(y_i|x)&=\\frac{e^{z_i(x)}}{\\sum_i e^{z_i(x)}},\n",
    "\\end{align}\n",
    "\n",
    "where $i$ runs from $1$ to $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\\begin{align}\n",
    "S^{k}(X)=\\theta^{k} \\cdot X\n",
    "\\end{align}\n",
    "All these vectors are typically stored as rows in a parameter matrix $\\Theta$.  \n",
    "After calculating all the classes\n",
    "\\begin{align}\n",
    "\\hat{p}^{k}=\\sigma(\\theta^{k} \\cdot X)=\\frac{\\exp(S^{k})}{\\sum_{j=1}^{K} S^{j}}\n",
    "\\end{align}\n",
    "\n",
    "Cross entropy:\n",
    "\\begin{align}\n",
    "J(\\Theta)=-\\frac{1}{m}\\sum_{i}\\sum_{j=1}^{K}y^{k}_{i}\\ln(\\hat{p}^{k}_{i})\n",
    "\\end{align}\n",
    "\n",
    "As can be guessed, the gradient of cross entropy reads:\n",
    "\\begin{align}\n",
    "\\partial_{\\theta^{k}_{j}}J(\\Theta)=&\\frac{1}{m}\\sum_{i}[\\hat{p}^{k}_{i}-y^{k}_{i}]x_{i}^{j}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of loss function in this case can be interpreted in terms of the cross entropy:\n",
    "\\begin{align}\n",
    "l=&-\\int dx p(x)\\int dy p(y|x)\\log\\hat{p}(y|x).\n",
    "\\end{align}\n",
    "In a sample of instance denotedy by $(x^i,y^i)$, the loss function is simply:\n",
    "\\begin{align}\n",
    "l({x,y})=&-\\frac{1}{N}\\log\\hat{p}(y^i|x^i),\n",
    "\\end{align}\n",
    "where only those $\\hat{p}(y^i|x^i)$ that have the same value as instance $(x^i,y^i)$ contributes to the loss function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
