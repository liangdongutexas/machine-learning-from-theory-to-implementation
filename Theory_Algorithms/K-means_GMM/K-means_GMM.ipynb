{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "The loss function of K-means is:\n",
    "\\begin{align}\n",
    "J=\\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}||x_n-u_k||^2,\n",
    "\\end{align}\n",
    "which is called distortion measure. $r_{nk}$ denotes which cluster the $nth$ data point belongs to. $r_{nk}=1$ if it belongs to the $kth$ cluster otherwise zero.\n",
    "The K-means algorithm is to find the $\\{r,\\mu\\}$ such that the loss function is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible algorithm to minimize the distortion measure is\n",
    "1. keep $\\mu$ fixed, find the value of ${r_{nk}}$ by assigning the data point to the cluster with least distance.\n",
    "2. update $\\mu_k$. In the two norm case, $\\mu_{k}=\\frac{\\sum_{n}r_{nk}x_n}{\\sum_{n}r_{nk}}$.\n",
    "\n",
    "Next, we realize the algorithm in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_means(object):\n",
    "    def __init__(self,K):\n",
    "        self.n_cluster=K\n",
    "    def fit(self,X,epochs):\n",
    "        X=np.array(X)\n",
    "        # pick n_cluster data points from X as the starting cluster centers\n",
    "        self.mu=X[np.random.randint(len(X), size=self.n_cluster)]\n",
    "        self.r_nk=self.predict(X)\n",
    "        for epoch in range(epochs):\n",
    "            # M step\n",
    "            self.mu=np.divide(np.matmul(np.transpose(self.r_nk),X),\\\n",
    "            np.expand_dims(np.sum(self.r_nk, axis=0),axis=1))\n",
    "            # E step\n",
    "            self.r_nk=self.predict(X)\n",
    "\n",
    "    def predict(self,X):\n",
    "        X=np.array(X)\n",
    "        r_nk=np.zeros(shape=(len(X),self.n_cluster))\n",
    "        for n in range(len(X)):\n",
    "            d=np.inf\n",
    "            K=0\n",
    "            for k in range(self.n_cluster):\n",
    "                d_new=np.sum(np.power(X[n]-self.mu[k],2))\n",
    "                if d_new<d:\n",
    "                    K=k\n",
    "                    d=d_new\n",
    "            r_nk[n,K]=1\n",
    "        return r_nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster1=np.random.randn(1000,2)+[0.5,0.5]\n",
    "cluster2=np.random.randn(1000,2)-[0.5,0.5]\n",
    "clusters=np.concatenate((cluster1,cluster2),axis=0)\n",
    "clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70628289,  0.77541215],\n",
       "       [-0.5924069 , -0.73523301]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_clf=k_means(2)\n",
    "k_clf.fit(clusters,90)\n",
    "k_clf.mu\n",
    "k_clf.predict([[0.5,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5924069 , -0.73523301],\n",
       "       [ 0.70628289,  0.77541215]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_clf=k_means(2)\n",
    "k_clf.fit(clusters,90)\n",
    "k_clf.mu\n",
    "k_clf.predict([[0.5,0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two examples, we can see the predicted clusters are permuted. This is due to we didn't specify which cluster should be assigned first. So permutation of $\\mu$s gives equivalent result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "The Gaussian Mixture Model assumed the distribution of data points satisfies the mixture of Gaussians:\n",
    "\\begin{align}\n",
    "p(x)=\\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(x|\\mu_k,\\Sigma_k).\n",
    "\\end{align}\n",
    "For $p(x)$ to be normalized, we have $\\sum_k{\\pi_k}=1$.And for $p(x)$ to be positive, we have $\\pi_k\\geq0$.\n",
    "Then it is instructive to imagine there is another random variable $Z$ which takes $K$ different values $\\{z_k\\}$ and has the distribution function $\\{\\pi_k\\}$. $Z$ is also called the latent variable. \n",
    "\n",
    "The above expression can be interpreted as\n",
    "\\begin{align}\n",
    "p(X)=\\sum_{Z}p(Z)p(X|Z),\n",
    "\\end{align}\n",
    "with $p(Z=z_k)=\\pi_k$ and $p(x|Z=z_k)=\\mathcal{N}(x|\\mu_k,\\Sigma_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the observed data points $\\{x_n\\}$, the goal is to maximize the likelihood of observing the data with respect to the unkonwn parameters $\\{\\pi,\\mu,\\Sigma\\}$ under the constraint $\\sum_k{\\pi_k}=1$. The logarithm of the likelihood function takes the following form:\n",
    "\\begin{align}\n",
    "\\log p({X}|\\pi,\\mu,\\Sigma)=\\sum_{n}\\log[\\sum_{k}\\pi_{k}\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)]+\\lambda (\\sum_k{\\pi_k}-1).\n",
    "\\end{align}\n",
    "We can then try to find the maximum by taking derivatives of the logarithm of the likelihood function (ll) with respect to each parameters:\n",
    "\\begin{align}\n",
    "\\frac{\\partial ll}{\\partial \\pi_k}=&\\sum_{n}\\frac{\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum_{q}\\pi_{q}\\mathcal{N}(x_n|\\mu_q,\\Sigma_q)}+\\lambda,\\\\\n",
    "\\frac{\\partial ll}{\\partial \\mu_k}=&\\sum_{n}\\frac{\\pi_{k} \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum_{q}\\pi_{q}\\mathcal{N}(x_n|\\mu_q,\\Sigma_q)}(x_n-\\mu_{k})\\Sigma_{k}^{-1},\\\\\n",
    "\\frac{\\partial ll}{\\partial \\Sigma_{k}^{-1}}=&\\sum_{n}\\frac{\\pi_{k} \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum_{q}\\pi_{q}\\mathcal{N}(x_n|\\mu_q,\\Sigma_q)}[-\\frac{1}{2}(x_n-\\mu_{k})(x_n-\\mu_{k})+\\frac{1}{2}\\Sigma].\\\\\n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\{\\pi,\\mu,\\Sigma\\}$ can be solved by setting the above derivatives to be zero. Noticing that the conditional probability $p(z_k|x)$ is:\n",
    "\\begin{align}\n",
    "p(z_k|x)=&\\frac{p(x|z_k)p(z_k)}{\\sum_{q}p(x|z_q)p(z_q)}\\\\\n",
    "        =&\\frac{\\mathcal{N}(x|\\mu_{k},\\Sigma_{k})\\pi_{k}}{\\sum_{q}\\mathcal{N}(x|\\mu_{q},\\Sigma_{q})\\pi_{q}},\n",
    "\\end{align}\n",
    "we have:\n",
    "\\begin{align}\n",
    "\\pi_k=&\\frac{N_k}{N},\\\\\n",
    "\\mu_k=&\\frac{1}{N_k}\\sum_{n}p(z_k|x_n)x_n,\\\\\n",
    "\\Sigma_{k}=&\\frac{1}{N_k}\\sum_{n}p(z_k|x_n)(x_n-\\mu_{k})(x_n-\\mu_{k}),\\\\\n",
    "\\end{align}\n",
    "where $N_k=\\sum_{n}p(z_k|x_n)$ which has the meaning of number of data points in the sample belonging to the cluster $k$ and $N$ is the sample length with $N=\\sum_{k}N_k$. The first expression tells us that the weight of the $k$th cluster is roughly the number of points belonging to that cluster over the total data points. The second expression shows that the center of the $k$th cluster is the probability weighted average of all data points vector. And the last expression shows the variance is the probability weighted average of all data variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above expression is not an explicit expression of the parameters at maximum of likelihood. Particularly the dependence of $p(z_k|x)$ on the parameters $\\{\\pi,\\mu,\\Sigma\\}$ makes it hard to solve for an explicit expression. However, the above expression gives an iteration approach:\n",
    "1. Initialize the parameters $\\{\\pi,\\mu,\\Sigma\\}$.\n",
    "2. E step: Calculate the conditional probability $p(z_k|x)$.\n",
    "3. M step: update the parameters according to the right hand side of the above expressions.\n",
    "4. Recursively repeat steps 2 and 3 untill some convergence criterion is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue in the code is how to properly generate a covariance matrix that is (1) symmetric, (2) positive definite, (3) invertible as required in the exponent of Gaussian distribution. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "There is a problem when calculating normal distribution that the normalization factor diverges when the stadard deviation is small.However, we can see that the normal distribution is always multiplied by $\\pi$. So the thought is to redefine $\\tilde{\\pi}_k\\equiv\\frac{\\pi_k}{\\sqrt{\\det(\\Sigma_k)}}$ such that the normalization factor is absorbed. Then the constraint becomes $\\sum_k{\\tilde{\\pi}_k}\\sqrt{\\det(\\Sigma_k)}=1$. Then in terms of the unnormalized normal distribution $\\tilde{\\mathcal{N}}$, $p(z_k|x)$ still reads:\n",
    "\\begin{align}\n",
    "p(z_k|x)=&\\frac{\\tilde{\\mathcal{N}}(x|\\mu_{k},\\Sigma_{k})\\tilde{\\pi}_{k}}{\\sum_{q}\\tilde{\\mathcal{N}}(x|\\mu_{q},\\Sigma_{q})\\tilde{\\pi}_{q}},\n",
    "\\end{align}\n",
    "Then the new iteration relations become:\n",
    "\\begin{align}\n",
    "\\tilde{\\pi}_k=&\\frac{N_k}{N\\sqrt{\\det(\\Sigma_k)}},\\\\\n",
    "\\mu_k=&\\frac{1}{N_k}\\sum_{n}p(z_k|x_n)x_n,\\\\\n",
    "\\Sigma_{k}=&\\frac{1}{N_k}\\sum_{n}p(z_k|x_n)(x_n-\\mu_{k})(x_n-\\mu_{k}),\\\\\n",
    "\\end{align}\n",
    "where $N_k=\\sum_{n}p(z_k|x_n)$ which has the meaning of number of data points in the sample belonging to the cluster $k$ and $N$ is the sample length with $N=\\sum_{k}N_k$. The first expression tells us that the weight of the $k$th cluster is roughly the number of points belonging to that cluster over the total data points. The second expression shows that the center of the $k$th cluster is the probability weighted average of all data points vector. And the last expression shows the variance is the probability weighted average of all data variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''random_cov=[]\n",
    "        for i in range(self.n_cluster):       \n",
    "            O=rvs(X.shape[1])\n",
    "            # O=np.random.randn(self.n_cluster,X.shape[1], X.shape[1])\n",
    "            random_cov.append(np.matmul(O.T,O))\n",
    "            \n",
    "        random_cov=np.array(random_cov)\n",
    "        try:\n",
    "            self.sigma=random_cov+sample_cov\n",
    "        except:\n",
    "            raise ValueError(random_cov.shape,sample_cov.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self,K):\n",
    "        self.n_cluster=K\n",
    "        \n",
    "    def fit(self,X,epochs):\n",
    "        X=np.array(X)\n",
    "        # pick n_cluster data points from X as the starting cluster centers\n",
    "        self.mus=X[np.random.randint(len(X), size=self.n_cluster)]\n",
    "        \n",
    "        # generate n_cluster covariance matricies by O^{T}O+(covariance matrix of the sample), where O is a random matrix\n",
    "        # with each element sampled from standard normal distribution\n",
    "        sample_cov=np.cov(X.T)  \n",
    "        self.sigmas=np.array([sample_cov for i in range(self.n_cluster)])\n",
    "\n",
    "        # generate \\pi\n",
    "        pi=np.random.randn(self.n_cluster)\n",
    "        pi=np.abs(pi)\n",
    "        self.pi=pi/np.sum(pi)\n",
    "                \n",
    "        for epoch in range(epochs):\n",
    "        # E process generating p(z|x)\n",
    "            gm=self.mix_gaussian(X,self.mus,self.sigmas,self.pi)        #shape=[sample,pi_k*Normal_dis_k]\n",
    "            p_z_given_x=np.divide(gm,np.sum(gm,axis=1)[:,np.newaxis])   #shape=[sample,p(z_k|x_n)]\n",
    "            N_k=np.sum(p_z_given_x,axis=0)                              #shape=[N_k]\n",
    "            p_zgx_over_N_k=np.divide(p_z_given_x,N_k)                   #shape=[sample,p(z_k|x_n)/N_k]\n",
    "\n",
    "        # M process updating self.mu,self.pi,self.sigma\n",
    "            #update self.pi\n",
    "            self.pi=np.sum(p_z_given_x,axis=0)/len(X)                 #shape=[pi_k]\n",
    "                      \n",
    "            #update self.mu\n",
    "            self.mus=np.zeros((self.n_cluster,X.shape[1]))\n",
    "            for p_n,x_n in zip(p_zgx_over_N_k,X):            \n",
    "                self.mus=np.add(self.mus,np.outer(p_n,x_n))\n",
    "                \n",
    "            #use updated mu to update self.sigma\n",
    "            self.sigmas=np.zeros((self.n_cluster,X.shape[1],X.shape[1]))\n",
    "            for p_n,x_n in zip(p_zgx_over_N_k,X):\n",
    "                # sigma_n stores for nth data point [K_cluster,dim(x_n-mu_k),dim(x_n-mu_k)]\n",
    "                sigma_n=[]\n",
    "                for p_n_k,mu_k in zip(p_n,self.mus):\n",
    "                    sigma_n.append(np.outer(np.outer(p_n_k,np.subtract(x_n,mu_k)),np.subtract(x_n,mu_k)))\n",
    "                self.sigmas=np.add(self.sigmas,np.array(sigma_n))\n",
    "                \n",
    "    def predict(self,X):\n",
    "        gm=self.mix_gaussian(X,self.mus,self.sigmas,self.pi)             #shape=[sample,pi_k*Normal_dis_k]\n",
    "        p_z_given_x=np.divide(gm,np.sum(gm,axis=1)[:,np.newaxis])      #shape=[sample,p(z_k|x_n)]\n",
    "        return np.argmax(p_z_given_x,axis=1)\n",
    "        \n",
    "    def mix_gaussian(self,X,mus,sigmas,pis):\n",
    "        #gm has the shape [sample_n,cluster_k], with each element pi_{k}\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)\n",
    "        X=np.array(X)\n",
    "        gm=np.array([])\n",
    "        for mu,sigma,pi in zip(mus,sigmas,pis):\n",
    "            try:\n",
    "                power=-np.matmul(np.subtract(mu,X)[:,np.newaxis,:],np.matmul(np.linalg.inv(sigma),np.subtract(mu,X)[:,:,np.newaxis]))             \n",
    "            except:\n",
    "                raise ValueError(np.linalg.inv(sigma))\n",
    "            power=power.reshape(power.shape[0],1)\n",
    "            try:\n",
    "                normal=np.exp(power)*pi/np.sqrt((2*np.pi)**X.shape[1]*np.absolute(np.linalg.det(sigma)))\n",
    "            except:\n",
    "                raise ValueError(np.linalg.det(sigma))\n",
    "            if len(gm)==0:\n",
    "                gm=np.exp(power)*pi\n",
    "            else:\n",
    "                gm=np.concatenate((gm,np.exp(power)*pi),axis=1)\n",
    "        return gm      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p_{nk}=\\pi_{k}\\mathcal{N}(x_n|\\mu_k,\\Sigma_k).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate two two-dimensional clusters\n",
    "cluster1=np.random.randn(1000,2)+[1,1]\n",
    "cluster2=np.random.randn(1000,2)-[1,1]\n",
    "clusters=np.concatenate((cluster1,cluster2),axis=0)\n",
    "clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.33197892, -1.25433492],\n",
       "       [ 0.81082361,  0.73892938]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_clf=GMM(2)\n",
    "gmm_clf.fit(clusters,epochs=100)\n",
    "gmm_clf.mus\n",
    "gmm_clf.predict([[1,1],[-1,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM in general\n",
    "The goal is to maximize the likelihood function:\n",
    "\\begin{align}\n",
    "p(\\boldsymbol{X}|\\theta)=\\sum_{\\boldsymbol{Z}}p(\\boldsymbol{X},\\boldsymbol{Z}|\\theta),\n",
    "\\end{align}\n",
    "which is expressed as the summation over the latent variables. The summation usually causes some difficulty, so we assume $p(\\boldsymbol{X},\\boldsymbol{Z}|\\theta)$ is easy to maximize while $p(\\boldsymbol{X}|\\theta)$ is not.\n",
    "We can rewrite it in another form as:\n",
    "\\begin{align}\n",
    "\\log p(\\boldsymbol{X}|\\theta)=\\sum_{\\boldsymbol{Z}} q(\\boldsymbol{Z}) \\log \\frac{p(\\boldsymbol{X},\\boldsymbol{Z}|\\theta)}{q(\\boldsymbol{Z})}+\\sum_{\\boldsymbol{Z}} q(\\boldsymbol{Z})\\log\\frac{q(\\boldsymbol{Z})}{p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta)}.\n",
    "\\end{align}\n",
    "The second term is the Kullback-Leibler divergence. For any fixed $q(\\boldsymbol{Z})$, we can variate $p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta)$ such that the minimal value is given by $p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta)=q(\\boldsymbol{Z})$ and Kullback-Leibler divergence becomes zero. Thus for any $q(\\boldsymbol{Z})$ and $p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta)$, $KL(q||p)\\geq0$ with zero given when they are equal.\n",
    "\n",
    "Then the first term gives a lower bond for the expression on the left hand side. In the next, we denote the first term as $\\mathcal{L}(q,\\theta)$, which is a functional on the distribution $q$ and a function on the parameter $\\theta$.\n",
    "* In the E step, for fixed parameter $\\theta_{old}$, we maximize the first term, which is equivalent to minimize the second term and the result is $q(\\boldsymbol{Z})=p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta_{old})$. Then the first term becomes $\\mathcal{L}(p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta),\\theta_{old})=\\sum_{\\boldsymbol{Z}} p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta_{old}) \\log \\frac{p(\\boldsymbol{X},\\boldsymbol{Z}|\\theta)}{p(\\boldsymbol{Z}|\\boldsymbol{X},\\theta_{old})}$.\n",
    "* In the M step, for fixed PDF $q$, we maximize $\\mathcal{L}$ by updating $\\theta$ and $q$ fixed. This is assumed to be viable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset, \n",
    "* In the E step, for fixed parameter we calculate \n",
    "\\begin{align}\n",
    "q(\\{\\boldsymbol{Z}\\})= & p(\\{\\boldsymbol{Z}_n\\}|\\{\\boldsymbol{X}_n\\},\\theta_{old})\\\\\n",
    "           \\propto & p(\\{\\boldsymbol{Z}_n\\},\\{\\boldsymbol{X}_n\\}|\\theta_{old})\\\\\n",
    "            \\propto& \\prod_n p(\\boldsymbol{Z}_n,\\boldsymbol{X}_n|\\theta_{old})\\\\\n",
    "                 = & \\frac{\\prod_n p(\\boldsymbol{Z}_n,\\boldsymbol{X}_n|\\theta_{old})}{\\sum_{\\{\\boldsymbol{Z}_n\\}}\\prod_n p(\\boldsymbol{Z}_n,\\boldsymbol{X}_n|\\theta_{old})}\\\\\n",
    "                 = & \\prod_n  \\frac{ p(\\boldsymbol{Z}_n,\\boldsymbol{X}_n|\\theta_{old})}{\\sum_{\\boldsymbol{Z}_n}p(\\boldsymbol{Z}_n,\\boldsymbol{X}_n|\\theta_{old})}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, it is convenient to define:\n",
    "$$\n",
    "q_n(\\boldsymbol{Z}_n)\\equiv   \\frac{ p(\\boldsymbol{Z}_n,\\boldsymbol{X}_n|\\theta_{old})}{\\sum_{\\boldsymbol{Z}}p(\\boldsymbol{Z},\\boldsymbol{X}_n|\\theta_{old})},\n",
    "$$\n",
    "such that: \n",
    "$$\n",
    "q(\\{\\boldsymbol{Z}\\})= \\prod_n q_n(\\boldsymbol{Z}_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the complete log likelihood function can be written as:\n",
    "\\begin{align}\n",
    "E(\\log p(\\{\\boldsymbol{X}\\}|\\theta))=& \\sum_{\\{\\boldsymbol{Z}\\}} q(\\{\\boldsymbol{Z}\\}) \\log p(\\{\\boldsymbol{X}\\},\\{\\boldsymbol{Z}\\}|\\theta)\\\\\n",
    "                                 =& \\sum_{\\{\\boldsymbol{Z}\\}} \\prod_m q_m(\\boldsymbol{Z}_m) \\sum_n \\log p(\\boldsymbol{X}_n,\\boldsymbol{Z}_n|\\theta)\\\\\n",
    "                                 =& \\sum_n \\sum_{\\boldsymbol{Z}}  q_n(\\boldsymbol{Z})  \\log p(\\boldsymbol{X}_n,\\boldsymbol{Z}_n|\\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Gaussian Mixture Model, we have\n",
    "\\begin{align}\n",
    "q_n(\\boldsymbol{Z}=\\boldsymbol{Z}_k)= &\\frac{p(\\boldsymbol{Z}=\\boldsymbol{Z}_k,\\boldsymbol{X}_n)}{\\sum_{\\boldsymbol{Z}_k}p(\\boldsymbol{Z}=\\boldsymbol{Z}_k,\\boldsymbol{X}_n)}\\\\\n",
    "                                    = & \\frac{\\mathcal{N}(\\boldsymbol{X}_n|\\mu^{old}_{k},\\Sigma^{old}_{k})\\pi^{old}_{k}}{\\sum_{q}\\mathcal{N}(\\boldsymbol{X}_n|\\mu^{old}_{q},\\Sigma^{old}_{q})\\pi^{old}_{q}}\n",
    "\\end{align}\n",
    "which desribes the probability that the given data instance $\\boldsymbol{X}_n$ belongs to cluster $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
